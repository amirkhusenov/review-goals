# Установка и настройка Ollama

Ollama — это инструмент для локального запуска больших языковых моделей (LLM). Это позволяет использовать AI без отправки данных на внешние серверы.

## Установка Ollama

### Windows

1. Скачайте установщик с официального сайта: https://ollama.com/download
2. Запустите установщик и следуйте инструкциям
3. После установки Ollama автоматически запустится как служба

### macOS

```bash
# Используя Homebrew
brew install ollama

# Или скачайте установщик с https://ollama.com/download
```

### Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## Запуск Ollama

После установки Ollama должен автоматически запуститься. Если нет, запустите вручную:

```bash
ollama serve
```

Ollama будет доступен по адресу `http://localhost:11434`

## Установка модели

Рекомендуемые модели для анализа целей:

### Llama 3.2 (рекомендуется)
```bash
ollama pull llama3.2
```

### Mistral (альтернатива)
```bash
ollama pull mistral
```

### Llama 3 (более мощная, но требует больше памяти)
```bash
ollama pull llama3
```

## Проверка установки

Проверьте, что Ollama работает:

```bash
ollama list
```

Должен показать список установленных моделей.

Протестируйте модель:

```bash
ollama run llama3.2 "Привет, как дела?"
```

## Настройка проекта

В проекте можно настроить модель через переменные окружения. Создайте файл `.env`:

```env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

## Требования к системе

- **Минимум**: 8 GB RAM для llama3.2
- **Рекомендуется**: 16 GB RAM для лучшей производительности
- **Для llama3**: минимум 16 GB RAM

## Устранение проблем

### Ollama не запускается

1. Проверьте, что порт 11434 свободен
2. Перезапустите службу Ollama
3. Проверьте логи: `ollama serve` (в терминале)

### Модель не отвечает

1. Убедитесь, что модель установлена: `ollama list`
2. Проверьте доступную память
3. Попробуйте другую модель (например, mistral вместо llama3.2)

### Медленная работа

1. Используйте более легкую модель (llama3.2 вместо llama3)
2. Закройте другие приложения, использующие память
3. Увеличьте объем RAM

## Альтернативные варианты

Если локальный запуск не подходит, можно использовать:

1. **OpenRouter API** — API для доступа к различным моделям
2. **Together AI** — облачный сервис для LLM
3. **Hugging Face Inference API** — бесплатный API для некоторых моделей

Для использования внешних API нужно изменить код в `app/server/api/review.post.ts`

